{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b  = [[]]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sgan.losses import gan_g_loss, gan_d_loss, l2_loss, l1_loss, length_normalized_l2_loss\n",
    "from sgan.losses import displacement_error, final_displacement_error\n",
    "\n",
    "from sgan.various_length_models import TrajectoryDiscriminator, LateAttentionFullGenerator\n",
    "from sgan.utils import int_tuple, bool_flag, get_total_norm\n",
    "from sgan.utils import relative_to_abs, get_dset_path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sgan.data.trajectories import TrajectoryDataset, seq_collate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dtypes(args):\n",
    "    long_dtype = torch.LongTensor\n",
    "    float_dtype = torch.FloatTensor\n",
    "    if args.use_gpu == 1:\n",
    "        long_dtype = torch.cuda.LongTensor\n",
    "        float_dtype = torch.cuda.FloatTensor\n",
    "    return long_dtype, float_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--pruning'], dest='pruning', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "FORMAT = '[%(levelname)s: %(filename)s: %(lineno)4d]: %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Dataset options\n",
    "parser.add_argument('--dataset_name', default='zara1', type=str)\n",
    "parser.add_argument('--delim', default=' ')\n",
    "parser.add_argument('--loader_num_workers', default=4, type=int)\n",
    "parser.add_argument('--obs_len', default=8, type=int)\n",
    "parser.add_argument('--pred_len', default=8, type=int)\n",
    "parser.add_argument('--skip', default=1, type=int)\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('--batch_size', default=64, type=int)\n",
    "parser.add_argument('--num_iterations', default=10000, type=int)\n",
    "parser.add_argument('--num_epochs', default=200, type=int)\n",
    "\n",
    "# Model Options\n",
    "parser.add_argument('--embedding_dim', default=64, type=int)\n",
    "parser.add_argument('--num_layers', default=1, type=int)\n",
    "parser.add_argument('--dropout', default=0, type=float)\n",
    "parser.add_argument('--batch_norm', default=0, type=bool_flag)\n",
    "parser.add_argument('--mlp_dim', default=1024, type=int)\n",
    "\n",
    "# Generator Options\n",
    "parser.add_argument('--encoder_h_dim_g', default=64, type=int)\n",
    "parser.add_argument('--decoder_h_dim_g', default=128, type=int)\n",
    "parser.add_argument('--noise_dim', default=None, type=int_tuple)\n",
    "parser.add_argument('--noise_type', default='gaussian')\n",
    "parser.add_argument('--noise_mix_type', default='ped')\n",
    "parser.add_argument('--clipping_threshold_g', default=0, type=float)\n",
    "parser.add_argument('--g_learning_rate', default=5e-4, type=float)\n",
    "parser.add_argument('--g_steps', default=1, type=int)\n",
    "\n",
    "# Pooling Options\n",
    "parser.add_argument('--pooling_type', default='pool_net')\n",
    "parser.add_argument('--pool_every_timestep', default=1, type=bool_flag)\n",
    "\n",
    "# Pool Net Option\n",
    "parser.add_argument('--bottleneck_dim', default=1024, type=int)\n",
    "\n",
    "# Social Pooling Options\n",
    "parser.add_argument('--neighborhood_size', default=2.0, type=float)\n",
    "parser.add_argument('--grid_size', default=8, type=int)\n",
    "\n",
    "# Discriminator Options\n",
    "parser.add_argument('--d_type', default='local', type=str)\n",
    "parser.add_argument('--encoder_h_dim_d', default=64, type=int)\n",
    "parser.add_argument('--d_learning_rate', default=5e-4, type=float)\n",
    "parser.add_argument('--d_steps', default=2, type=int)\n",
    "parser.add_argument('--clipping_threshold_d', default=0, type=float)\n",
    "\n",
    "# Loss Options\n",
    "parser.add_argument('--intention_loss_weight', default=0, type=float)\n",
    "parser.add_argument('--intention_loss_type', default='l2', type=str)\n",
    "parser.add_argument('--l2_loss_weight', default=0, type=float)\n",
    "parser.add_argument('--resist_loss_weight', default=0, type=float)\n",
    "parser.add_argument('--heading_loss_weight', default=0, type=float)\n",
    "parser.add_argument('--best_k', default=1, type=int)\n",
    "\n",
    "# Output\n",
    "parser.add_argument('--output_dir', default=os.getcwd())\n",
    "parser.add_argument('--print_every', default=5, type=int)\n",
    "parser.add_argument('--checkpoint_every', default=100, type=int)\n",
    "parser.add_argument('--checkpoint_name', default='checkpoint')\n",
    "parser.add_argument('--checkpoint_start_from', default=None)\n",
    "parser.add_argument('--restore_from_checkpoint', default=1, type=int)\n",
    "parser.add_argument('--num_samples_check', default=5000, type=int)\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--use_gpu', default=1, type=int)\n",
    "parser.add_argument('--timing', default=0, type=int)\n",
    "parser.add_argument('--gpu_num', default=\"0\", type=str)\n",
    "parser.add_argument('--plot_dir', default=\"../plots/\")\n",
    "parser.add_argument('--benchmark', default=False, type=bool)\n",
    "parser.add_argument('--spatial_dim', default=True, type=bool)\n",
    "parser.add_argument('--pruning', default=False, type=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " args = parser.parse_args(\"--dataset_name zara1\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    args.delta\n",
    "except AttributeError:\n",
    "     args.delta = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(args, path, shuffle=True, min_ped=1):\n",
    "    dset = TrajectoryDataset(\n",
    "        path,\n",
    "        obs_len=args.obs_len,\n",
    "        pred_len=args.pred_len,\n",
    "        skip=args.skip,\n",
    "        delim='\\t', min_ped=min_ped)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=args.loader_num_workers,\n",
    "        collate_fn=seq_collate)\n",
    "    return dset, loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'zara1'\n",
    "train_path = get_dset_path(dataset_name, 'train')\n",
    "val_path = get_dset_path(dataset_name, 'val')\n",
    "\n",
    "long_dtype, float_dtype = get_dtypes(args)\n",
    "\n",
    "logger.info(\"Initializing train dataset\")\n",
    "train_dset, train_loader = data_loader(args, train_path)\n",
    "logger.info(\"Initializing val dataset\")\n",
    "_, val_loader = data_loader(args, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "\n",
    "for batch in train_loader:\n",
    "    batch = [tensor.cuda() for tensor in batch]\n",
    "    (obs_traj, pred_traj_gt, obs_traj_rel, pred_traj_gt_rel, non_linear_ped,\n",
    "     loss_mask, seq_start_end, goals, goals_rel) = batch\n",
    "    for start, end in seq_start_end:\n",
    "        if count==1:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = end.item()\n",
    "start = start.item()\n",
    "end_pos = obs_traj_rel[-1,start:end,:]\n",
    "num_ped = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def row_repeat( tensor, num_reps):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        -tensor: 2D tensor of any shape\n",
    "        -num_reps: Number of times to repeat each row\n",
    "        Outpus:\n",
    "        -repeat_tensor: Repeat each row such that: R1, R1, R2, R2\n",
    "        \"\"\"\n",
    "        col_len = tensor.size(1)\n",
    "        tensor = tensor.unsqueeze(dim=1).repeat(1, num_reps, 1)\n",
    "        tensor = tensor.view(-1, col_len)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_ped = end - start\n",
    "end_pos = obs_traj_rel[-1, start:end, :]\n",
    "\n",
    "# r1,r1,r1, r2,r2,r2, r3,r3,r3 - r1,r2,r3, r1,r2,r3, r1,r2,r3\n",
    "end_pos_difference = row_repeat(end_pos, num_ped) - end_pos.repeat(num_ped, 1)\n",
    "end_displacement = obs_traj_rel[-1,start:end,:] - obs_traj_rel[-2,start:end,:] / 0.4\n",
    "end_speed = torch.sqrt(torch.sum(end_displacement**2, dim=1)).view(-1,1)\n",
    "\n",
    "end_speed_difference =  row_repeat(end_speed, num_ped) - end_speed.repeat(num_ped, 1)\n",
    "end_heading = torch.atan2(end_displacement[:,0], end_displacement[:,1]).view(-1,1)\n",
    "end_heading_difference =  row_repeat(end_heading, num_ped) - end_heading.repeat(num_ped, 1)\n",
    "# num_ped**2\n",
    "delta_distance = torch.sqrt(torch.sum(end_pos_difference**2, dim=1)).view(-1,1)\n",
    "# num_ped\n",
    "delta_speed = torch.abs(end_speed_difference)\n",
    "delta_heading = torch.abs(torch.atan2(torch.sin(end_heading_difference), torch.cos(end_heading_difference)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(seq_start_end[:,1] - seq_start_end[:,0]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.cat((delta_distance, delta_speed, delta_heading),1)\n",
    "_, _, p = svm_predict([], a.tolist(), m,'-b 1')\n",
    "p = torch.FloatTensor(p)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (p>0.5).long().view(num_ped, num_ped)\n",
    "mask.unsqueeze(2).repeat(1,1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracking group related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, time, copy, numpy\n",
    "# Parameters\n",
    "growthAmount = 0.5 / 3.5   # growth of relation strength (between 0.0 and 1.0) per second\n",
    "slowDecayAmount  = 0.5 / 40.0 # slow decay of relation strength (between 0.0 and 1.0) per second\n",
    "fastDecayAmount  = 0.5 / 5.0  # fast decay, when maximum distance is exceeded\n",
    "maxDistance = 4.0  # distance above which relation strength will be forced to 0.0\n",
    "g_maxDistance = 3.0\n",
    "g_maxSpeedDifference =1.0\n",
    "g_maxOrientationDifference = math.pi/4\n",
    "g_minSpeedToConsiderOrientation=0.1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ped = end-start\n",
    "i=1\n",
    "for t1Index in range(0, num_ped):\n",
    "    for t2Index in range(t1Index+1, num_ped):\n",
    "        p1= obs_traj_rel[i,t1Index, :]\n",
    "        p2= obs_traj_rel[i,t2Index, :]\n",
    "        v1 = obs_traj_rel[i,t1Index, :] - obs_traj_rel[i-1,t1Index, :]\n",
    "        v2 = obs_traj_rel[i,t2Index, :] - obs_traj_rel[i-1,t2Index, :]\n",
    "        speed1 = torch.sqrt(v1[0]**2 + v1[1]**2) / 0.4\n",
    "        speed2 = torch.sqrt(v2[0]**2 + v2[1]**2) / 0.4\n",
    "        theta1 = torch.atan2(v1[0], v1[1]) if speed1 >= g_minSpeedToConsiderOrientation else 0\n",
    "        theta2 = torch.atan2(v2[0], v2[1]) if speed2 >= g_minSpeedToConsiderOrientation else 0\n",
    "        distance = torch.sqrt((p1[0] - p2[0])**2 + (p1[1]-p2[1])**2)\n",
    "        deltaspeed = torch.abs(speed1 - speed2)\n",
    "        deltaangle = torch.abs(torch.atan2(torch.sin(theta1-theta2), torch.cos(theta1-theta2)))\n",
    "\n",
    "            \n",
    "            #Gating for large distance, very different velocities, or very different angle\n",
    "        if (distance > g_maxDistance or  deltaspeed > g_maxSpeedDifference  or deltaangle > g_maxOrientationDifference):\n",
    "            \n",
    "            positiveRelationProbability = 0.1;\n",
    "            negativeRelationProbability = 0.9;\n",
    "\n",
    "            \n",
    "        else:\n",
    "                # Prepare SVM classifier\n",
    "                g_svmNode[0].value = distance;\n",
    "                g_svmNode[1].value = deltaspeed;\n",
    "                g_svmNode[2].value = deltaangle;\n",
    "\n",
    "                // Run SVM classifier\n",
    "                double probabilityEstimates[2];\n",
    "                svm_predict_probability(g_svmModel, g_svmNode, probabilityEstimates);\n",
    "                positiveRelationProbability = probabilityEstimates[0];\n",
    "                negativeRelationProbability = probabilityEstimates[1];\n",
    "            }\n",
    "\n",
    "            // Store results for this pair of tracks\n",
    "            SocialRelation socialRelation;\n",
    "            socialRelation.type = SocialRelation::TYPE_SPATIAL;\n",
    "            socialRelation.strength = positiveRelationProbability;\n",
    "            socialRelation.track1_id = t1.track_id;\n",
    "            socialRelation.track2_id = t2.track_id;\n",
    "\n",
    "            socialRelations->elements.push_back(socialRelation);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Publish spatial relations\n",
    "    g_socialRelationsPublisher.publish(socialRelations);\n",
    "}\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/xinjiey/Group_Navi_GAN/env/lib/python3.5/site-packages/libsvm'\n",
    "sys.path.append(path)\n",
    "from svmutil import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = svm_load_model('//data/xinjiey/Group_Navi_GAN/spencer/group/social_relationships/groups_probabilistic_small.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_svmNode = [svm_node(1,0), svm_node(2,0), svm_node(3,0), svm_node(-1,0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,p = svm_predict([], [[0.3,0.4,3.14]], m,'-b 1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heading related "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs_traj[-1,0,:])\n",
    "print(obs_traj[0,0,:])\n",
    "print(obs_traj_rel[-1,0,:])\n",
    "print(obs_traj_rel[0,0,:])\n",
    "if(abs(obs_traj_rel[0,0,0] - obs_traj_rel[2,0,0]) <2):\n",
    "    print(\"ass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_x = obs_traj_rel[0,:, 0]  - obs_traj_rel[-1,:, 0] \n",
    "delta_y = obs_traj_rel[0,:, 1]  - obs_traj_rel[-1,:, 1] \n",
    "theta = torch.atan2(delta_x, delta_y)\n",
    "theta.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.empty(2,2)\n",
    "nn.init.eye_(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_heading_difference(obs_traj_rel, start, end):\n",
    "    heading_mask = nn.init.eye_(torch.empty(end-start, end-start))\n",
    "    delta_x = obs_traj_rel[0,start:end, 0]  - obs_traj_rel[-1,start:end, 0] \n",
    "    delta_y = obs_traj_rel[0,start:end, 1]  - obs_traj_rel[-1,start:end, 1] \n",
    "    theta = torch.atan2(delta_x, delta_y)\n",
    "    for t in range(0,end-start-1):\n",
    "        for p in range(t+1, end-start):\n",
    "            angle = abs(torch.atan2(torch.sin(theta[t]-theta[p]), torch.cos(theta[t]-theta[p])))\n",
    "            heading_mask[t,p] = heading_mask[p,t] =torch.cos(angle)\n",
    "    rr = heading_mask.unsqueeze(0).repeat(8,1,1)\n",
    "    return rr\n",
    "\n",
    "for start, end in seq_start_end.data:\n",
    "    heading_mask  = get_heading_difference(obs_traj_rel, start, end).cuda()\n",
    "\n",
    "    for t in range(start, end):\n",
    "        if t == start:\n",
    "            distance = pred_traj_gt[:,t+1:end,:].clone()\n",
    "            mask  = heading_mask [:, 0, 1:].clone()\n",
    "        elif t == end-1:\n",
    "            distance = pred_traj_gt[:,start:t,:].clone()\n",
    "            mask  = heading_mask [:, -1, :-1].clone()\n",
    "        else:\n",
    "            distance = torch.cat((pred_traj_gt[:,start:t,:], pred_traj_gt[:,t+1:end,:]), 1).clone()\n",
    "            mask = torch.cat((heading_mask[:, t-start, 0:t-start], heading_mask [:,t-start, t+1-start:]), 1).clone() # 8*seq\n",
    "\n",
    "        distance -= pred_traj_gt[:,t,:].view(-1,1,2)\n",
    "      \n",
    "        distance = 0.45 - torch.sqrt(torch.sum(distance**2, dim=2)) - 0.15*mask\n",
    "        resist_loss = distance[distance > 0.]\n",
    "        print(resist_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_mask = get_heading_difference(obs_traj_rel, 36, 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_traj_gt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_traj_rel[-3,:, 0] .size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_traj.view(-1,2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = nn.Linear(2,64).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_traj_embedding = se(obs_traj.view(-1,2).cuda()).view(-1, obs_traj.size(1), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_traj_embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hidden( batch=689):\n",
    "    return (\n",
    "        torch.zeros(1, batch, 64).cuda(),\n",
    "        torch.zeros(1, batch, 64).cuda()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.LSTM(64,64, 1, dropout=0.0).cuda()\n",
    "state_tuple = init_hidden()\n",
    "output, state = encoder(obs_traj_embedding, state_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_mlp(dim_list, activation='relu', batch_norm=True, dropout=0):\n",
    "    layers = []\n",
    "    for dim_in, dim_out in zip(dim_list[:-1], dim_list[1:]):\n",
    "        layers.append(nn.Linear(dim_in, dim_out))\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(dim_out))\n",
    "        if activation == 'relu':\n",
    "            layers.append(nn.ReLU())\n",
    "        elif activation == 'leakyrelu':\n",
    "            layers.append(nn.LeakyReLU())\n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "    return nn.Sequential(*layers).cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_heading_difference(obs_traj_rel, _start, _end, dim):\n",
    "    obs_length = 8\n",
    "    start = _start\n",
    "    end = _end\n",
    "    #logger.info('[get_heading_difference]: obs_length is {}, count is {}'.format(obs_traj_rel.size(0), obs_length))\n",
    "    heading_mask = nn.init.eye_(torch.empty(end-start, end-start))\n",
    "    delta_x = obs_traj_rel[0,start:end, 0]  - obs_traj_rel[-1,start:end, 0] \n",
    "    delta_y = obs_traj_rel[0,start:end, 1]  - obs_traj_rel[-1,start:end, 1] \n",
    "    theta = torch.atan2(delta_x, delta_y)\n",
    "    for t in range(0,end-start-1):\n",
    "        for p in range(t+1, end-start):\n",
    "            angle = abs(torch.atan2(torch.sin(theta[t]-theta[p]), torch.cos(theta[t]-theta[p])))\n",
    "            heading_mask[t,p] = heading_mask[p,t] =torch.cos(angle)\n",
    "    mask = heading_mask.unsqueeze(2).repeat(1,1,dim).cuda()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PoolHiddenNet(nn.Module):\n",
    "    \"\"\"Pooling module as proposed in our paper\"\"\"\n",
    "    def __init__(\n",
    "        self, embedding_dim=16, h_dim=32, mlp_dim=64, bottleneck_dim=32,\n",
    "        activation='relu', batch_norm=True, dropout=0.0\n",
    "    ):\n",
    "        super(PoolHiddenNet, self).__init__()\n",
    "\n",
    "        self.mlp_dim = 1024\n",
    "        self.h_dim = h_dim\n",
    "        self.bottleneck_dim = bottleneck_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        mlp_pre_dim = embedding_dim + h_dim\n",
    "        mlp_pre_pool_dims = [mlp_pre_dim, 512, bottleneck_dim]\n",
    "\n",
    "        self.spatial_embedding = nn.Linear(2, embedding_dim).cuda()\n",
    "        self.mlp_pre_pool = make_mlp(\n",
    "            mlp_pre_pool_dims,\n",
    "            activation=activation,\n",
    "            batch_norm=batch_norm,\n",
    "            dropout=dropout)\n",
    "\n",
    "    def repeat(self, tensor, num_reps):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        -tensor: 2D tensor of any shape\n",
    "        -num_reps: Number of times to repeat each row\n",
    "        Outpus:\n",
    "        -repeat_tensor: Repeat each row such that: R1, R1, R2, R2\n",
    "        \"\"\"\n",
    "        col_len = tensor.size(1)\n",
    "        tensor = tensor.unsqueeze(dim=1).repeat(1, num_reps, 1)\n",
    "        tensor = tensor.view(-1, col_len)\n",
    "        return tensor\n",
    "\n",
    "    def forward(self, h_states, seq_start_end, end_pos):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - h_states: Tensor of shape (num_layers, batch, h_dim)\n",
    "        - seq_start_end: A list of tuples which delimit sequences within batch\n",
    "        - end_pos: Tensor of shape (batch, 2)\n",
    "        Output:\n",
    "        - pool_h: Tensor of shape (batch, bottleneck_dim)\n",
    "        \"\"\"\n",
    "        pool_h = []\n",
    "        for _, (start, end) in enumerate(seq_start_end):\n",
    "            start = start.item()\n",
    "            end = end.item()\n",
    "            print(\"start, end:\"+str(start)+\",\"+str(end))\n",
    "            num_ped = end - start\n",
    "            curr_hidden = h_states.view(-1, self.h_dim)[start:end]\n",
    "            curr_end_pos = end_pos[start:end]\n",
    "            # Repeat -> H1, H2, H1, H2\n",
    "            curr_hidden_1 = curr_hidden.repeat(num_ped, 1)\n",
    "            # Repeat position -> P1, P2, P1, P2\n",
    "            curr_end_pos_1 = curr_end_pos.repeat(num_ped, 1)\n",
    "            # Repeat position -> P1, P1, P2, P2\n",
    "            curr_end_pos_2 = self.repeat(curr_end_pos, num_ped)\n",
    "            curr_rel_pos = curr_end_pos_1 - curr_end_pos_2\n",
    "            mask = get_heading_difference(obs_traj_rel, start, end, self.bottleneck_dim)\n",
    "            print(\"curr_rel_pos\" + str(curr_rel_pos.size()))\n",
    "            curr_rel_embedding = self.spatial_embedding(curr_rel_pos)\n",
    "            print(\"curr_rel_embedding \" + str(curr_rel_embedding.size()))\n",
    "            mlp_h_input = torch.cat([curr_rel_embedding, curr_hidden_1], dim=1)\n",
    "            print(\"mlp_h_input \" + str( mlp_h_input.size()))\n",
    "            curr_pool_h = self.mlp_pre_pool(mlp_h_input)\n",
    "            print(\"curr_pool_h \" + str( curr_pool_h.size()))\n",
    "           # print(\"curr_pool_h.view(num_ped, num_ped, -1).max(1) \" + str( curr_pool_h.view(num_ped, num_ped, -1).max(1))\n",
    "            curr_pool_h = curr_pool_h.view(num_ped, num_ped, -1).mul(mask).max(1)[0]\n",
    "            print(\"curr_pool_h \" + str( curr_pool_h.size()))\n",
    "            pool_h.append(curr_pool_h)\n",
    "        pool_h = torch.cat(pool_h, dim=0)\n",
    "        return pool_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(3,3,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_net = PoolHiddenNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_heading_difference(obs_traj_rel, 0, 3, 32).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_pos = obs_traj[-1, :, :]\n",
    "force_final_encoder_h = state[0]\n",
    "pool_h = pool_net(force_final_encoder_h, seq_start_end, end_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp11 =[ [row[1]] for row in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x+y for x,y in zip(b[1],tmp11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/home/asus/torch_gpu_ros_3.6/biwi_eth_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time = np.genfromtxt(file_name, usecols=(0))\n",
    "userid = np.genfromtxt(file_name, usecols=(1))\n",
    "x = np.genfromtxt(file_name, usecols=(2))\n",
    "y = np.genfromtxt(file_name, usecols=(3))\n",
    "active_peds_id = np.unique(userid)\n",
    "times = np.unique(all_time)\n",
    "active_peds_id.sort()\n",
    "times.sort()\n",
    "    allY = np.zeros((len(times), len(addresses)))\n",
    "    allX = np.zeros((len(times), 2))\n",
    "    strengths={}\n",
    "    for address, j in zip(addresses, list(range(len(addresses)))):\n",
    "        ind = np.nonzero(address==macaddress)\n",
    "        temp_strengths=strength[ind]\n",
    "        temp_x=x[ind]\n",
    "        temp_y=y[ind]\n",
    "        temp_times = all_time[ind]\n",
    "        for time in temp_times:\n",
    "            vals = time==temp_times\n",
    "            if any(vals):\n",
    "                ind2 = np.nonzero(vals)\n",
    "                i = np.nonzero(time==times)\n",
    "                allY[i, j] = temp_strengths[ind2]\n",
    "                allX[i, 0] = temp_x[ind2]\n",
    "                allX[i, 1] = temp_y[ind2]\n",
    "    X = allX[:, :]\n",
    "Y = allY[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_queue.append({'active_peds_id': active_peds_id,\n",
    "                                 'peds_pos_t': peds_pos_t,\n",
    "                                 'time_stamp': scanTime})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
